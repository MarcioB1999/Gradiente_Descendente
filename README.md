# Gradiente Descendente

## Algoritmo

O m√©todo gradiente consiste basicamente em um m√©todo iterativo que a partir de um ponto inicial $\textbf{x}^{0}=\left(x_{1}^{0},...,x_{n}^{0}\right) \in \mathbf{R}^{n}$, percorremos a dire√ß√£o do vetor gradiente sucessivamente de modo a obter o m√≠nimo local de uma fun√ß√£o $f\left(\textbf{x}\right)$.

<img height="180em" src="https://github.com/MarcioB1999/Gradiente_Descendente/blob/main/imgs/gradienteMetodo.png"/>

O funcionamento do algortimo pode-se ver na imagem anterior, retirada do artigo https://www.researchgate.net/publication/351361317_Ottimizzazione_di_flussi_esterni_ed_interni_mediante_metodi_CFD_adjoint_e_mesh_morphing, no qual o pr√≥ximo ponto da itera√ß√£o √© dado pela seguinte equa√ß√£o,

$\textbf{x}^{k+1}=\textbf{x}^{k}-\alpha \bigtriangledown f\left(\text{x}^{k}\right)$

ou temos

$x_{i}^{k+1}=x_{i}^{k}-\alpha \frac{\partial}{\partial x_{i}}f\left(\textbf{x}^{i}\right),\forall i=1,...,n$

no qual $\alpha$ denomina-se taxa de aprendizado, e √© exatamente o quanto que queremos percorrer na dire√ß√£o do vetor gradiente. 
## Crit√©rio de Parada
O crit√©rio de parada pode ser tr√™s, que s√£o escolhidos atrav√©s de um par√¢metro, os crit√©rios s√£o

(1) $\mid\frac{f\left(\textbf{x}^{k+1}\right)-f\left(\textbf{x}^{k}\right)}{f\left(\textbf{x}^{k}\right)}\mid\leq\varepsilon$

(2) $\mid \mid x^{k+1}-x^{k}\mid \mid \leq\varepsilon$ 

(3) $\mid \mid \bigtriangledown f\left(\text{x}^{k+1}\right) \mid \mid \leq\varepsilon$

com $\varepsilon$ pr√≥ximo de zero quanto o usu√°rio desejar.

## Regra Wolfe

A regra wolfe se dar para aumentar o desempenho do m√©todo, ele se dar pela escolha do par√¢metros $\alpha$, em vez de um valor fixo. Portanto, para cada itera√ß√£o $k$ do algoritmo, se escolhe $\alpha_{k}$, de modo a satisfazer as desigualdades (1.1) e (1.2).

(1.1) $f\left(x^{k}-\alpha^{k}\bigtriangledown f\left(\text{x}^{k}\right)\right)\leq f\left(x^{k}\right)-\alpha_{k}\theta_{1}\mid \mid \bigtriangledown f\left(\text{x}^{k}\right)\mid \mid$

(1.2) $-\langle \bigtriangledown f\left(\text{x}^{k+1}\right),\bigtriangledown f\left(\text{x}^{k}\right)\rangle \geq -\theta_{2}\mid\mid \bigtriangledown f\left(\text{x}^{k}\right)\mid\mid $

OBS:Por enquanto o m√©todo n√£o se dar bem com fun√ß√µes que crescem rapidamente. Oque pode tentar solucionar, ser√° mudan√ßa no tipode dados, para uns mais altos, ou uma melhor escolha dos pontos iniciais, ou pelomenos que n√£o sejam t√£o grandes.

## C√≥digo

Basicamente para se ter uma int√¢ncia da classe, o construtor ir√° receber um objeto da classe fun√ß√£o, https://github.com/MarcioB1999/Interpretador_funcao, e a quantidade de vari√°veis da fun√ß√£o(em breve n√£o ser√° mais necess√°rio). Para o usu√°rio, apenas se tem acesso ao m√©todo Descida_Gradiente

### Descida_Gradiente (Double taxa_aprendizagem, Int parada ):
>####     Argumentos:
>>#####    Taxa_aprendizagem: O quanto se quer percorrer na dire√ß√£o do gradiente.
>>##### Parada: N√∫mero m√°ximo de itera√ß√µes.
>#### Retorno: Retorna um ponteiro double para o vetor $\textbf{x}^{k}=\min_{\textbf{x}} f\left(\textbf{x}\right)$ (Nesse caso tambem se refere a m√≠nimos locais, n√£o apenas m√≠nimos globais).


## OBS:
ainda esta em constru√ß√£o üòÄ
mas praticamente quase la, ele sofre limita√ß√µes pela classe fun√ß√£o recebida como argumento.
